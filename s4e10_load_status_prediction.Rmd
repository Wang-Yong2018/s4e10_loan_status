---
title: "s4e10_load_approval_status"
author: "WangYong"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# load data
```{r}
library(tidyverse)
library(tidymodels)
#library( ParBayesOptimization)
library(themis)
competition_name <- 'playground-series-s4e10'
data_path <- '../input/playground-series-s4e10/'
train <- read_csv(file.path(data_path, "train.csv"),show_col_types=F) |> 
  mutate(loan_status=as.factor(loan_status)) 
test <- read_csv(file.path(data_path, "test.csv"),show_col_types=F) 


combined_df <- bind_rows(list('train'=train,'test'=test),.id ='source' )

sample_submission <- read_csv(file.path(data_path, "sample_submission.csv"),show_col_types = FALSE) 
```
# EDA
## quick skim
```{r}
train|> ggplot(aes(x=loan_status))+geom_bar()
skimr::skim(train)
skimr::skim(test)
```

# split the data 
```{r}


split <- initial_split(train, prop = 0.8, strata = loan_status)
train_data <- training(split)
test_data <- testing(split)

cv_folds <- vfold_cv(train_data, v = 10, strata = loan_status)

```

# feature engineering 
## recipes
### baseline
```{r}
rcp_baseline <-
  recipe(loan_status ~ ., data = train_data) |>
  update_role(id, new_role='ID')|>
  step_mutate(loan_status=as.factor(loan_status),skip=T)|>
  
  step_impute_median(all_numeric_predictors())|> 
  step_log(all_numeric_predictors(),offset = 1, skip = FALSE) |>
  
  step_unknown(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_nzv(all_predictors())|>
  step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> # Scale numeric predictors
  step_upsample(loan_status, over_ratio = 1)|>
  check_missing(all_predictors())

#rcp_baseline |>prep()|>juice() |>glimpse()
```

#### 方差校验（train & test 比较）
```{r}
get_var_compare <- function(rcp,is_debug=F){
  
  train_var <- rcp|> 
    prep()|>bake(new_data=train)|>
    select(-loan_status, -id) |> # 去除目标变量和ID变量
    summarise_all(var)|>
    pivot_longer(cols = everything(),
               names_to = "feature",
               values_to = "variance")
    
  test_var  <- rcp|>
    prep()|>bake(new_data=test)|>
    select( -id) |> # 去除目标变量和ID变量
    summarise_all(var)|>
    pivot_longer(cols = everything(),
               names_to = "feature",
               values_to = "variance")
  
  compared_result<-
    train_var |> 
    left_join(test_var, by='feature')|>
    mutate(variance_ratio = round(variance.x / variance.y,2))
  
  nfeature_var_changed <- 
    compared_result|>
    filter(variance_ratio >1*1.5 ) |>
    nrow()
 
  if (is_debug){
    print(compared_result)
    
  } 
  return(nfeature_var_changed)
}


```
### vrose - baseline 
```{r}
rcp_bs_rose <-
  recipe(loan_status ~ ., data = train_data) |>
  update_role(id, new_role='ID', )|>
  step_rm(id,skip=TRUE)|>
  step_mutate(loan_status=as.factor(loan_status),skip=T)|>
  
  step_impute_median(all_numeric_predictors())|> 
  step_log(all_numeric_predictors(),offset = 1, skip = FALSE) |>
  
  step_unknown(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_nzv(all_predictors())|>
  step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> # Scale numeric predictors
  step_rose(loan_status,over_ratio = 1,skip=TRUE)|>
  check_missing(all_predictors())

rcp_bs_rose |> get_var_compare()
```

### v1 bad - baseline + age + income + loan_amnt
first plot eda for age, income and loan_amnt
```{r}
train |>
  select(person_age, person_income, loan_amnt) |>
  pivot_longer(everything(), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = value)) +
  geom_density() +
  theme_minimal()+
  facet_wrap(~variable, ncol = 3, scales = "free")
```

```{r}
rcp_bs_v1 <-
  recipe(loan_status ~ ., data = train_data) |>
  update_role(id, new_role='ID', )|>
  step_mutate(loan_status=as.factor(loan_status),skip=T)|>
    # Step 1: Handle Outliers in person_age (Winsorizing)
  step_mutate(person_age = ifelse(person_age > 80, 80,person_age) ) %>% #Winsorizing step
  # Step 2: Log transformation for Income and Loan Amount
  #step_log(person_income, base = 10) %>%  #Use the log transformation
  #step_log(loan_amnt, base = 10) %>% #Use the log transformation
  step_log(all_numeric_predictors(),base=10,offset = 1, skip = FALSE) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_unknown(all_nominal_predictors()) |>
  step_impute_median(all_numeric_predictors())|> 
  step_nzv(all_predictors())|>
  step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> # Scale numeric predictors
  step_rose(loan_status, over_ratio = 1)|>
  check_missing(all_predictors())

get_var_compare(rcp_bs_v1)
```
### v2 baseline + interative fe
```{r}
rcp_bs_v2 <-
  recipe(loan_status ~ ., data = train_data) |>
  update_role(id, new_role='ID', )|>
  step_mutate(loan_status=as.factor(loan_status),skip=T)|>
  step_mutate(interest_rate = loan_amnt * loan_int_rate,
              #: Total interest paid might be insightful.
              debt_rate = loan_amnt / person_income , 
              #: Debt-to-income ratio (a variation).
              age_emp_rate = person_age * person_emp_length
              #: Experience can be a factor in loan repayment.
  )|>
  step_log(all_numeric_predictors(),offset = 1, skip = FALSE) |>
  step_normalize(all_numeric_predictors()) |>
  step_unknown(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_impute_median(all_numeric_predictors())|>
  step_nzv(all_predictors())|>
  step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> # Scale numeric predictors
  step_rose(loan_status, over_ratio = 1,skip = TRUE)|>
  check_missing(all_predictors())


get_var_compare(rcp_bs_v2)
```
### v3 baseline + smote
```{r}
rcp_bs_v3 <-
  recipe(loan_status ~ ., data = train_data) |>
  update_role(id, new_role='ID', )|>
  step_mutate(loan_status=as.factor(loan_status),skip=T)|>
  step_log(all_numeric_predictors(),offset = 1, skip = FALSE) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_unknown(all_nominal_predictors()) |>
  step_impute_median(all_numeric_predictors())|> 
  step_nzv(all_predictors())|>
  step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> # Scale numeric predictors
  step_rose(loan_status, over_ratio = 1,skip = TRUE)|>
  check_missing(all_predictors())

rcp_bs_v3 |>get_var_compare()
```
### v4 baseline + spline of loan amount
```{r}
rcp_bs_v4 <-
  recipe(loan_status ~ ., data = train_data) |>
  update_role(id, new_role='ID', )|>
  step_mutate(loan_status=as.factor(loan_status),skip=T)|>
  
  step_impute_median(all_numeric_predictors())|>
  
  step_bs(loan_amnt, 
          deg_free = 3, 
          options=list(knots = c(5000, 10000, 15000, 20000,25000)
                       ))|> # Set degrees of freedom to 5; set knots
  step_log(all_numeric_predictors(),offset = 1, skip = FALSE) |>
  
  step_unknown(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_nzv(all_predictors())|>
  step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> # Scale numeric predictors
  step_rose(loan_status, over_ratio = 1,skip=TRUE)|>
  check_missing(all_predictors())

rcp_bs_v4 |>get_var_compare()
```
### v5 baseline + interactive + spline of loan amount
```{r}
rcp_bs_v5 <-
  recipe(loan_status ~ ., data = train_data) |>
  update_role(id, new_role='ID', )|>
  step_mutate(loan_status=as.factor(loan_status),skip=T)|>
  
  step_impute_median(all_numeric_predictors())|> 
  step_mutate(interest_rate = loan_amnt * loan_int_rate,
              #: Total interest paid might be insightful.
              debt_rate = loan_amnt / person_income , 
              #: Debt-to-income ratio (a variation).
              age_emp_rate = person_age * person_emp_length
              #: Experience can be a factor in loan repayment.
  )|>
  step_bs(loan_amnt, 
          deg_free = 3, 
          options=list(knots = c(5000, 10000, 15000, 20000,25000)
                       ))|> # Set degrees of freedom to 5; set knots
  step_log(all_numeric_predictors(),offset = 1, skip = FALSE) |>
  
  step_unknown(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_nzv(all_predictors())|>
  step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> # Scale numeric predictors
  step_rose(loan_status, over_ratio = 1,skip=TRUE)|>
  check_missing(all_predictors())

rcp_bs_v5 |>get_var_compare()
```
### selected_recipes
```{r}
selected_list <- 
  list(baseline = rcp_baseline,
       bs_rose = rcp_bs_rose,
       bs_v3 = rcp_bs_v3,
       bs_v4 = rcp_bs_v4,
       bs_v5 = rcp_bs_v5
       )
```


# modeling 
## model engine setup
```{r}
  
library(bonsai)
library(lightgbm)

glm_model <- 
  logistic_reg() |>
  set_engine("glm", family = "binomial")|>
  set_mode('classification')

lgbm_model <- 
  boost_tree(
    trees = 500,
    tree_depth = 6,
    learn_rate =  0.1,
    min_n = 100,
    loss_reduction = 0.001,
    stop_iter = 10
  ) %>% 
  set_engine(engine = "lightgbm",
             is_unbalance = TRUE,
             metric='auc',
             num_leaves = 30,
             num_threads = 12,
             verbose=1
             #      boosting = "goss"   # this may slow the system
  ) %>%
  set_mode(mode = "classification")

roc_metrics <- metric_set(accuracy, precision, recall, f_meas, roc_auc)

ctrl <- control_resamples(save_pred = TRUE,
                          save_workflow = TRUE,
                          verbose=TRUE,
                          allow_par= TRUE
                          )
```



## fit
### glm
```{r}
library(future)
plan(multisession,workers = 8)

# Now run fit_resamples()`...
glm_fit_list <-
  selected_list |>
  map(\(x) workflow() |>
           add_model(glm_model)|>
           add_recipe(x)|>
           fit_resamples(cv_folds, control = ctrl, metrics = roc_metrics))

fit_result_df <- 
  glm_fit_list |>
  map_dfr(\(x) collect_metrics(x),.id = 'wflow_id')
fit_result_df|>filter(.metric=='roc_auc')

plan(sequential)

```

#### tune glm
```{r}
# 定义模型
tune_glm_model <- logistic_reg(penalty = tune(),  # 正则化强度, 需要调参
                               mixture = tune()) %>%  # L1/L2 混合比例, 需要调参
  set_engine("glmnet",family = "binomial") %>%  # 使用 glmnet 引擎
  set_mode("classification")  # 分类模式

tune_glm_grid <- grid_regular(penalty(range = c(-4, 1)),  # penalty 的搜索范围 (log10 尺度)
                             mixture(range = c(0, 1)),    # mixture 的搜索范围
                             levels = 5) #  每个参数的取值数量

# 训练并调参
library(future)
plan(multisession,workers = 8)

tune_glm_wflow <- 
  workflow() %>%
  add_recipe(rcp_bs_v5) %>%
  add_model(tune_glm_model)
  
tune_glm_results <- 
 tune_glm_wflow|>
  tune_grid(resamples =cv_folds,       # 交叉验证 folds
            grid = tune_glm_grid,          # 参数网格
            metrics = metric_set(roc_auc)) #  评估指标

plan(sequential)
print(tune_glm_results|>collect_metrics()|>arrange(desc(mean)))

best_glm_params <- tune_glm_results %>%
  select_best(metric="roc_auc") #  选择 AUC 最高的参数组合


# 选择最佳参数
best_logistic_params <- 
  tune_glm_results |>
  select_best(metric="roc_auc") #  选择 AUC 最高的参数组合
# 使用最佳参数更新 Workflow
final_logistic_workflow <- 
  tune_glm_wflow %>%
  finalize_workflow(parameters=best_logistic_params)

# 训练最终模型
final_logistic_fit <- final_logistic_workflow %>%
  fit(data = train)

```

### lightgbm
```{r}
library(future)
plan(multisession,workers = 8)

lgbm_fit_list <-
  selected_list |>
  map(\(x) workflow() |>
           add_model(lgbm_model)|>
           add_recipe(x)|>
           fit_resamples(cv_folds, control = ctrl, metrics = roc_metrics))

fit_result_df <- 
  lgbm_fit_list |>
  map_dfr(\(x) collect_metrics(x),.id = 'wflow_id')
plan(sequential)
fit_result_df|>filter(.metric=='roc_auc')
```

#### tune
```{r}
# 3. LightGBM Model Specification
tune_lgbm_model <- boost_tree( trees = tune(),               # Number of trees - tune() means we'll optimize
                               tree_depth = tune(),          # Max tree depth - important for regularization
                               learn_rate = tune(),          # Learning rate - shrinkage, reduce overfit
                               loss_reduction = 0.01,      # Min loss reduction - regularization
                               #sample_size = tune(),         # Bagging fraction (rows)
                               #mtry = tune(),                # Colsample_bytree -  Feature subsampling
                               min_n = tune()                # Min. terminal node size - regularization
                               ) %>%
  set_engine("lightgbm") %>%
  set_mode("classification")    # IMPORTANT:  Classification! (or "regression")

# 4. Workflow Creation
tune_lgbm_workflow <- workflow() %>%
  add_recipe(rcp_bs_v5) %>%
  add_model(tune_lgbm_model)

# 5. Tuning Grid Definition
# Revised Tuning Grid Definition
tune_lgbm_grid <- 
  tune_lgbm_workflow %>% 
  extract_parameter_set_dials() %>%
  update(trees=trees(c(300L,500L)),
         min_n=min_n(c(30,100)),
         learn_rate=learn_rate(c(-2,-1)),
         #sample_size=sample_size(c(0.7,0.9))
         )|>
    grid_space_filling(size = 10)
  
# 6. Resampling Strategy
# Using 5-fold cross-validation, repeated 3 times for more robust results.

# 7. Tuning the Model
#  Use tune_grid to find the best hyperparameter combo
#  Control the number of cpus if you only want to use x -1 cpus
#  If this is too much running locally, you decrease size from the tuning grid and repeats from the resampling strategy

library(future)
plan(multisession,workers = 8)

lgbm_tune_results <- 
  tune_lgbm_workflow %>%
  tune_grid(
    resamples = cv_folds,
    grid = tune_lgbm_grid,
    metrics = metric_set(roc_auc ), # More metrics!
    control = control_grid(save_pred = TRUE, 
                           verbose = TRUE,
                           allow_par = TRUE) # Keep predictions
  )

plan(sequential)
# 8. Explore Tuning Results
# Show the best results:
lgbm_tune_results %>%
  collect_metrics()

# Select the best model based on ROC AUC
best_lgbm_params <- lgbm_tune_results %>%
  select_best(metric="roc_auc")

# Plotting Tune Results

plot <- lgbm_tune_results %>%    # Plotting the ROC AUC
      collect_metrics() %>%
      ggplot(aes(x = trees , y = mean)) +
      geom_point() +
      geom_line()
plot

# 9. Finalize Workflow
# Apply the best hyperparameter values to the workflow
final_lgbm_workflow <- tune_lgbm_workflow %>%
  finalize_workflow(best_lgbm_params) # Putting the best model into the workflow

# 10. Train Final Model
# Train the final LightGBM model on the entire training dataset
final_lgbm_model <- final_lgbm_workflow %>%
  fit(data = train)

# 11. Feature Importance
#  Visualize Feature importance
library(vip)
final_lgbm_model %>%
  extract_fit_parsnip() %>%
  vip(num_features = 20, aesthetics = list(fill = "darkgreen", alpha = 0.8))  # Plot top 20


```

## workflowsets
```{r}
library(workflowsets)
# all_metrics 定义了要计算的评估指标 (准确率, 精确率, 召回率, F1-score, AUC)


race_results <- 
  workflows <-
  workflow_set(
    preproc = selected_list,
    models = list(glm = glm_model, lgbm = lgbm_model)
  )|>
  workflow_map(
    "fit_resamples", #快速对比模型方案
    resamples = cv_folds,
    metrics = metric_set(accuracy, precision, recall, f_meas, roc_auc), #roc_auc需要classProbs=TRUE
    verbose = TRUE )

print(race_results|>collect_metrics()) #展示模型结果

best_workflow_id <- race_results %>%
  rank_results() %>%                                       # 按指定指标排序
  filter(.metric == "roc_auc") %>%
  select(wflow_id, .metric, mean , rank) %>% #展示指定的模型
  filter(rank==1) %>%
  pull(wflow_id)

# 11. 获取选择最佳模型对应的工作流
best_workflow <- workflows %>%
  extract_workflow(best_workflow_id)
# 12. 把最佳模型应用到测试数据集,评估泛化能力
final_results <- last_fit(best_workflow,
                         split =split,
                         )

# 13.输出测试集上的泛化性能
collect_metrics(final_results)
```
```{r}
# best_param <- select_best(tune_results, metric) # or other `select_*()`
#  wflow <- finalize_workflow(wflow, best_param)  # or just `finalize_model()`
#  wflow_fit <- fit(wflow, data_set)
final_model <- best_workflow|>fit(train)
```

# final_model
```{r}
final_fit <-
  workflow() |>
  add_model(lgbm_model)|>
  add_recipe(rcp_bs_v4)|>
  fit(data=train)


final_result <- final_fit |> predict( train, type='prob')|>
           bind_cols(train|>mutate(loan_status=factor(loan_status, levels = c(0, 1))))

final_result |> 
           roc_auc(loan_status, .pred_0,event_level = 'first')

final_roc_curve <-
  final_result|> 
           roc_curve(loan_status, .pred_0,event_level = 'first')
  
final_roc_curve |> autoplot(x)

```

# predict

```{r}
best_fit <-stack_fit
test_predictions <- predict(best_fit, test, type = "class")

submission <- sample_submission %>%
mutate(loan_status = test_predictions$.pred_class)

write_csv(submission, "submission.csv")
```

# submit to kaggle
```{r}
# submit latest submission.csv
system('kaggle competitions submit -c playground-series-s4e10 -f submission.csv -m "stack 0.92 (only lgmb)"')
Sys.sleep(5)
# get latest score 
system('kaggle competitions submissions -q -c playground-series-s4e10')
```

# combined eda
## combine train & test
```{r}
#| fig.height: 24
#| fig.width: 6 

combined_df <- bind_rows(list('train'=train,'test'=test),.id ='source' )
combined_df |>glimpse( )
num_fe <- combined_df|>select(where(is.numeric)) |>select(-id)|>names()
text_fe <-combined_df|>select(where(is.character))|>select(-source)|>names()
```

## plot eda
```{r}
combined_df |>
  pivot_longer(cols=num_fe)|>
  ggplot(aes(x= log1p(value),fill=source))+
  geom_density(alpha=0.5)+
  facet_wrap(~name,ncol = 1,scales = 'free')

```


## model based compare 对抗验证，检验训练和测试数据是否存在显著差异
```{r}
# 2. 定义一个 Recipe,  这里需要根据你的数据进行调整
recipe_adv <- recipe(source ~ ., data = combined_df) %>%
  # update_role(id, new_role = "ID") %>%
  # step_dummy(all_nominal_predictors()) %>% # 对名义变量做one-hot encoding
  # step_zv(all_predictors()) # 移除零方差变量
 update_role(id, new_role='ID', )|>
  step_rm(loan_status)|>
  step_mutate(source=as.factor(source),skip=T)|>
  step_log(all_numeric_predictors(),offset = 1, skip = FALSE) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_unknown(all_nominal_predictors()) |>
  step_impute_median(all_numeric_predictors())|> 
  step_nzv(all_predictors())|>
  step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> # Scale numeric predictors
  #step_upsample(loan_status, over_ratio = 1)|>
  check_missing(all_predictors())


# 3. 定义模型 (例如,使用logistic regression)
logistic_model <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# 4. 创建 workflow
workflow_adv <- workflow() %>%
  add_recipe(recipe_adv) %>%
  add_model(logistic_model)

# 5. 训练并评估模型 (可以使用交叉验证)
cv_folds <- vfold_cv(combined_df, v = 10)

results_adv <- workflow_adv %>%
  fit_resamples(resamples = cv_folds, metrics = metric_set(roc_auc))

collect_metrics(results_adv)

```
 AS the combined_df train & test roc_auc is almost equal to 0.50, there is no significant difference between train & test data distribution
 
## stacking 
```{r}
library(stacks)
# 定义控制参数
ctrl_grid <- control_stack_grid()  # 使用control_stack()设置控制参数

# 定义基模型
model_spec_glm <- logistic_reg() %>% set_engine("glm",family = "binomial")%>% set_mode("classification")
model_spec_rf <- rand_forest() %>% set_engine("ranger") %>% set_mode("classification")
model_spec_xgb <- boost_tree() %>% set_engine("xgboost") %>% set_mode("classification")
model_spec_lgb <- boost_tree() %>% set_engine("lightgbm") %>% set_mode("classification")

library(future)
plan(multisession,workers = 8)

# 训练基模型
wf_set <- 
  workflow_set(preproc = list(rcp_bs_v5),  # 使用相同的预处理流程
                            models = list(glm = model_spec_glm,
                                          rf = model_spec_rf, 
                                          xgb = model_spec_xgb,
                                          lgb=model_spec_lgb)
                            ) 

wf_set_trained  <- 
  wf_set |>
  option_add( control = control_stack_grid(),
              metrics = metric_set(roc_auc) )|>
  workflow_map( fn="tune_grid",
                resamples =cv_folds,
                verbose = TRUE)




# 初始化堆叠
stack_model <- stacks() %>%
  add_candidates(wf_set_trained)

# 拟合元模型
stack_fit <- stack_model %>%
  blend_predictions() %>%  # 选择最优基模型组合
  fit_members()  # 训练元模型
plan(sequential)
```


```{r}
autoplot(stack_fit)
autoplot(stack_fit, type = "weights")

collect_parameters(stack_fit,'recipe_lgb')

stack_preds <- 
  test_data %>%
  select(loan_status) %>%
  bind_cols(predict(stack_fit, test_data, class='prob', members = TRUE))

stack_preds|>accuracy(truth=loan_status,.pred_class)

```

